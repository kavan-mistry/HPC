{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# HPC assignment 17"
      ],
      "metadata": {
        "id": "b6Xz-QbP_Gc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIVrIFmPLg5P",
        "outputId": "40c92924-3647-42b4-da10-61f100be72b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Apr 16 07:06:05 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. To print hello message on the screen using kernal function"
      ],
      "metadata": {
        "id": "8ym1IHRFLqOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hello_1_1.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void cuda_hello_1_1() {\n",
        "    printf(\"Hello World from GPU with grid dimension (1, 1) and block dimension (1, 1)!\\n\");\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    cuda_hello_1_1<<<1,1>>>();\n",
        "    cudaDeviceSynchronize(); // Make sure all GPU work is done before exiting\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuY-YyhYLP-9",
        "outputId": "95472387-2e46-4942-bc54-f5ca55b937e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hello_1_1.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o hello_1_1 hello_1_1.cu"
      ],
      "metadata": {
        "id": "DSCY4EAILTZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./hello_1_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLUUZdxJLZQv",
        "outputId": "0df66078-4932-445c-c879-03e890a47c18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World from GPU with grid dimension (1, 1) and block dimension (1, 1)!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. To add two vectors of size 100 and 20000 and analyze the performance comparison between cpu and gpu processing"
      ],
      "metadata": {
        "id": "wyPc75yXL526"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPU"
      ],
      "metadata": {
        "id": "ZScCWhmRQ5rB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcvJganBVAHV",
        "outputId": "c814ad05-edee-46f2-9734-de86afcf7d0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pycuda\n",
            "  Downloading pycuda-2024.1.tar.gz (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytools>=2011.2 (from pycuda)\n",
            "  Downloading pytools-2024.1.1-py2.py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.1/85.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from pycuda) (1.4.4)\n",
            "Collecting mako (from pycuda)\n",
            "  Downloading Mako-1.3.3-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (4.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (4.11.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from mako->pycuda) (2.1.5)\n",
            "Building wheels for collected packages: pycuda\n",
            "  Building wheel for pycuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2024.1-cp310-cp310-linux_x86_64.whl size=661204 sha256=51efb7c5582dd86e48b9404a05e0a366352406f4840bf4dc162fe9a89aa2ad1c\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/34/d2/9a349255a4eca3a486d82c79d21e138ce2ccd90f414d9d72b8\n",
            "Successfully built pycuda\n",
            "Installing collected packages: pytools, mako, pycuda\n",
            "Successfully installed mako-1.3.3 pycuda-2024.1 pytools-2024.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "import time"
      ],
      "metadata": {
        "id": "CixF6vnyL4os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CUDA kernel function to add two vectors\n",
        "cuda_kernel_code = \"\"\"\n",
        "__global__ void vector_add(float *a, float *b, float *c, int n) {\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (i < n) {\n",
        "        c[i] = a[i] + b[i];\n",
        "    }\n",
        "}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2vegVoEzPO5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the CUDA kernel code\n",
        "cuda_module = SourceModule(cuda_kernel_code)\n",
        "\n",
        "# Get a reference to the CUDA kernel function\n",
        "vector_add_cuda = cuda_module.get_function(\"vector_add\")"
      ],
      "metadata": {
        "id": "g2UkVmumPO0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_add_gpu(a, b):\n",
        "    n = a.size\n",
        "\n",
        "    # Create device arrays\n",
        "    a_gpu = cuda.mem_alloc(a.nbytes)\n",
        "    b_gpu = cuda.mem_alloc(b.nbytes)\n",
        "    c_gpu = cuda.mem_alloc(b.nbytes)\n",
        "\n",
        "    # Copy data to device\n",
        "    cuda.memcpy_htod(a_gpu, a)\n",
        "    cuda.memcpy_htod(b_gpu, b)\n",
        "\n",
        "    # Define block and grid dimensions\n",
        "    block_dim = (256, 1, 1)\n",
        "    grid_dim = ((n + block_dim[0] - 1) // block_dim[0], 1)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Launch the CUDA kernel\n",
        "    vector_add_cuda(a_gpu, b_gpu, c_gpu, np.int32(n), block=block_dim, grid=grid_dim)\n",
        "\n",
        "    # Synchronize threads to ensure all output is calculated\n",
        "    cuda.Context.synchronize()\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Copy result back to host\n",
        "    c = np.empty_like(a)\n",
        "    cuda.memcpy_dtoh(c, c_gpu)\n",
        "\n",
        "    return c, end_time - start_time"
      ],
      "metadata": {
        "id": "XJnuci7QPOug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_size_1 = 100\n",
        "vector_size_2 = 20000\n",
        "a = np.random.randn(vector_size_2).astype(np.float32)\n",
        "b = np.random.randn(vector_size_2).astype(np.float32)\n",
        "\n",
        "# Perform vector addition on GPU\n",
        "result_gpu1, gpu_time1 = vector_add_gpu(a[:vector_size_1], b[:vector_size_1])\n",
        "result_gpu2, gpu_time2 = vector_add_gpu(a[:vector_size_2], b[:vector_size_2])"
      ],
      "metadata": {
        "id": "nOMGUEfRPOrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vector addition of size\", vector_size_1, \"on GPU took\", gpu_time1, \"seconds.\")\n",
        "\n",
        "print(\"Vector addition of size\", vector_size_2, \"on GPU took\", gpu_time2, \"seconds.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPZ3W9TpPOpE",
        "outputId": "c7dca35a-ab21-49ff-f27a-4bdf7c23a0b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector addition of size 100 on GPU took 0.0007643699645996094 seconds.\n",
            "Vector addition of size 20000 on GPU took 6.818771362304688e-05 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CPU"
      ],
      "metadata": {
        "id": "hvEheCs9QnP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time"
      ],
      "metadata": {
        "id": "nO6v1mvxQPAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_add_cpu(a, b):\n",
        "    start_time = time.time()\n",
        "    result = a + b\n",
        "    end_time = time.time()\n",
        "    return result, end_time - start_time"
      ],
      "metadata": {
        "id": "zo1Vg8WJQO8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_size_1 = 100\n",
        "vector_size_2 = 20000\n",
        "a = np.random.randn(vector_size_2).astype(np.float32)\n",
        "b = np.random.randn(vector_size_2).astype(np.float32)\n",
        "\n",
        "# Perform vector addition on CPU\n",
        "result_cpu1, cpu_time1 = vector_add_cpu(a[:vector_size_1], b[:vector_size_1])\n",
        "result_cpu2, cpu_time2 = vector_add_cpu(a[:vector_size_2], b[:vector_size_2])"
      ],
      "metadata": {
        "id": "ylRC8U3yQO5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vector addition of size\", vector_size_1, \"on CPU took\", cpu_time1, \"seconds.\")\n",
        "print(\"Vector addition of size\", vector_size_2, \"on CPU took\", cpu_time2, \"seconds.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvyOvmPVREnC",
        "outputId": "5b372129-324c-431d-f347-050e7423be1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector addition of size 100 on CPU took 2.3365020751953125e-05 seconds.\n",
            "Vector addition of size 20000 on CPU took 1.9311904907226562e-05 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Vector addition of size 100 on CPU took 2.384185791015625e-05 seconds.\n",
        "* Vector addition of size 20000 on CPU took 1.9788742065429688e-05 seconds."
      ],
      "metadata": {
        "id": "OfAueaUNTxNI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Vector addition of size 100 on GPU took 0.0007691383361816406 seconds.\n",
        "* Vector addition of size 20000 on GPU took 7.128715515136719e-05 seconds."
      ],
      "metadata": {
        "id": "TjE0ERXeXL_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. To multply two matrix of size 20 X 20 and 1024 X 1024 analyze the performance comparison between cpu and gpu processing"
      ],
      "metadata": {
        "id": "XlFZnJPSMCNU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPU"
      ],
      "metadata": {
        "id": "ktQT63sESAbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def matrix_multiply_gpu(a, b):\n",
        "    # Define CUDA kernel code for matrix multiplication\n",
        "    cuda_code = \"\"\"\n",
        "    __global__ void matrix_multiply(float *a, float *b, float *c, int n) {\n",
        "        int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "        int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "        if (row < n && col < n) {\n",
        "            float sum = 0.0;\n",
        "            for (int i = 0; i < n; ++i) {\n",
        "                sum += a[row * n + i] * b[i * n + col];\n",
        "            }\n",
        "            c[row * n + col] = sum;\n",
        "        }\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    # Compile CUDA kernel code\n",
        "    mod = SourceModule(cuda_code)\n",
        "\n",
        "    # Get kernel function\n",
        "    matrix_multiply_cuda = mod.get_function(\"matrix_multiply\")\n",
        "\n",
        "    # Allocate memory on device\n",
        "    a_gpu = cuda.mem_alloc(a.nbytes)\n",
        "    b_gpu = cuda.mem_alloc(b.nbytes)\n",
        "    c_gpu = cuda.mem_alloc(a.nbytes)\n",
        "\n",
        "    # Copy input matrices to device\n",
        "    cuda.memcpy_htod(a_gpu, a)\n",
        "    cuda.memcpy_htod(b_gpu, b)\n",
        "\n",
        "    # Define grid and block dimensions\n",
        "    block_size = (16, 16, 1)\n",
        "    grid_size = ((a.shape[1] + block_size[0] - 1) // block_size[0], (a.shape[0] + block_size[1] - 1) // block_size[1], 1)\n",
        "\n",
        "    # Call CUDA kernel\n",
        "    matrix_multiply_cuda(a_gpu, b_gpu, c_gpu, np.int32(a.shape[0]), block=block_size, grid=grid_size)\n",
        "\n",
        "    # Copy result back to host\n",
        "    c = np.empty_like(a)\n",
        "    cuda.memcpy_dtoh(c, c_gpu)\n",
        "\n",
        "    return c"
      ],
      "metadata": {
        "id": "tJV-oQtaMJjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate random matrices\n",
        "def generate_random_matrix(rows, cols):\n",
        "    return np.random.rand(rows, cols).astype(np.float32)\n"
      ],
      "metadata": {
        "id": "MH0PTY5uR93A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to measure time taken for matrix multiplication\n",
        "def measure_time(matrix_size, func, *args):\n",
        "    start_time = time.time()\n",
        "    result = func(*args)\n",
        "    end_time = time.time()\n",
        "    return result, end_time - start_time"
      ],
      "metadata": {
        "id": "ARcmhxX4R9hH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sizes of matrices to be multiplied\n",
        "matrix_sizes = [(20, 20), (1024, 1024)]"
      ],
      "metadata": {
        "id": "wYUVcf1TV3NU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for size in matrix_sizes:\n",
        "    print(f\"\\nMatrix size: {size}\")\n",
        "    a = generate_random_matrix(*size)\n",
        "    b = generate_random_matrix(*size)\n",
        "\n",
        "    # GPU matrix multiplication\n",
        "    gpu_result, gpu_time = measure_time(size, matrix_multiply_gpu, a, b)\n",
        "    print(f\"GPU time: {gpu_time:.6f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhDhR0szV3CK",
        "outputId": "4f4362ab-4763-475e-8da2-349439534094"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Matrix size: (20, 20)\n",
            "GPU time: 0.428407 seconds\n",
            "\n",
            "Matrix size: (1024, 1024)\n",
            "GPU time: 0.018636 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CPU"
      ],
      "metadata": {
        "id": "V1qyw1pBSImQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CPU matrix multiplication\n",
        "def matrix_multiply_cpu(a, b):\n",
        "    result = np.zeros((a.shape[0], b.shape[1]), dtype=np.float32)\n",
        "    for i in range(a.shape[0]):\n",
        "        for j in range(b.shape[1]):\n",
        "            for k in range(a.shape[1]):\n",
        "                result[i, j] += a[i, k] * b[k, j]\n",
        "    return result"
      ],
      "metadata": {
        "id": "iodrO8o3R9c8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate random matrices\n",
        "def generate_random_matrix(rows, cols):\n",
        "    return np.random.rand(rows, cols).astype(np.float32)"
      ],
      "metadata": {
        "id": "YBPrjBRTR9Z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to measure time taken for matrix multiplication\n",
        "def measure_time(matrix_size, func, *args):\n",
        "    start_time = time.time()\n",
        "    result = func(*args)\n",
        "    end_time = time.time()\n",
        "    return result, end_time - start_time"
      ],
      "metadata": {
        "id": "2npE0fjiSSlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sizes of matrices to be multiplied\n",
        "matrix_sizes = [(20, 20), (1024, 1024)]"
      ],
      "metadata": {
        "id": "nSreHNxRu7iT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for size in matrix_sizes:\n",
        "    print(f\"\\nMatrix size: {size}\")\n",
        "    a = generate_random_matrix(*size)\n",
        "    b = generate_random_matrix(*size)\n",
        "\n",
        "    # CPU matrix multiplication\n",
        "    cpu_result, cpu_time = measure_time(size, matrix_multiply_cpu, a, b)\n",
        "    print(f\"CPU time: {cpu_time:.6f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZHw0TVRu-Tg",
        "outputId": "f670cf12-1063-48bc-cadb-2d3687e26bfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Matrix size: (20, 20)\n",
            "CPU time: 0.004824 seconds\n",
            "\n",
            "Matrix size: (1024, 1024)\n",
            "CPU time: 704.230331 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* CPU Time for 1024: 0.12308359146118164 seconds\n",
        "* CPU Time for 20: 0.0019140243530273438 seconds"
      ],
      "metadata": {
        "id": "c-K4-CpFT46o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Matrix size: (20, 20)\n",
        "* GPU time: 0.703994 seconds\n",
        "\n",
        "* Matrix size: (1024, 1024)\n",
        "* GPU time: 0.014648 seconds"
      ],
      "metadata": {
        "id": "840b6ln5XRuy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. To obtain CUDA device information and print the output"
      ],
      "metadata": {
        "id": "QOLoudChMMdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pycuda.driver as cuda\n",
        "\n",
        "# Initialize PyCUDA\n",
        "cuda.init()\n",
        "\n",
        "# Get the number of CUDA devices\n",
        "num_devices = cuda.Device.count()\n",
        "\n",
        "print(\"Number of CUDA devices:\", num_devices)\n",
        "\n",
        "# Iterate over each CUDA device and print its properties\n",
        "for i in range(num_devices):\n",
        "    device = cuda.Device(i)\n",
        "    print(\"\\nCUDA Device:\", i)\n",
        "    print(\"  Name:\", device.name())\n",
        "    print(\"  Compute Capability:\", device.compute_capability())\n",
        "    print(\"  Total Memory:\", device.total_memory() / (1024 ** 3), \"GB\")\n",
        "    print(\"  Max Threads per Block:\", device.max_threads_per_block)\n",
        "    print(\"  Multiprocessor Count:\", device.multiprocessor_count)\n",
        "    print(\"  Clock Rate:\", device.clock_rate / 1e6, \"GHz\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZHjK0EQMwA0",
        "outputId": "ee6096c7-07de-4a1f-b494-5deeb8aa3a6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of CUDA devices: 1\n",
            "\n",
            "CUDA Device: 0\n",
            "  Name: Tesla T4\n",
            "  Compute Capability: (7, 5)\n",
            "  Total Memory: 14.74810791015625 GB\n",
            "  Max Threads per Block: 1024\n",
            "  Multiprocessor Count: 40\n",
            "  Clock Rate: 1.59 GHz\n"
          ]
        }
      ]
    }
  ]
}