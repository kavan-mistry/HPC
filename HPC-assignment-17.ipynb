{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6Xz-QbP_Gc0"
   },
   "source": [
    "# HPC assignment 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1713241996594,
     "user": {
      "displayName": "SAL19IT 6023kavanmistry",
      "userId": "03477421122245380763"
     },
     "user_tz": -330
    },
    "id": "OIVrIFmPLg5P",
    "outputId": "5fe534fc-e505-4280-862a-e9dd8daffea5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 16 12:31:26 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 552.12                 Driver Version: 552.12         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   47C    P4             13W /   50W |       0MiB /   8188MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ym1IHRFLqOq"
   },
   "source": [
    "## 1. To print hello message on the screen using kernal function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1713241930805,
     "user": {
      "displayName": "SAL19IT 6023kavanmistry",
      "userId": "03477421122245380763"
     },
     "user_tz": -330
    },
    "id": "kuY-YyhYLP-9",
    "outputId": "3e784caf-3e8f-449c-e260-26d0a7c6bace"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hello_1_1.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile hello_1_1.cu\n",
    "\n",
    "#include <stdio.h>\n",
    "\n",
    "__global__ void cuda_hello_1_1() {\n",
    "    printf(\"Hello World from GPU with grid dimension (1, 1) and block dimension (1, 1)!\\n\");\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    cuda_hello_1_1<<<1,1>>>();\n",
    "    cudaDeviceSynchronize(); // Make sure all GPU work is done before exiting\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 1387,
     "status": "ok",
     "timestamp": 1713241956132,
     "user": {
      "displayName": "SAL19IT 6023kavanmistry",
      "userId": "03477421122245380763"
     },
     "user_tz": -330
    },
    "id": "DSCY4EAILTZ8"
   },
   "outputs": [],
   "source": [
    "!nvcc -o hello_1_1 hello_1_1.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 648,
     "status": "ok",
     "timestamp": 1713241978792,
     "user": {
      "displayName": "SAL19IT 6023kavanmistry",
      "userId": "03477421122245380763"
     },
     "user_tz": -330
    },
    "id": "XLUUZdxJLZQv",
    "outputId": "34c23813-8749-47e0-c24c-3bb718d8fca7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World from GPU with grid dimension (1, 1) and block dimension (1, 1)!\n"
     ]
    }
   ],
   "source": [
    "!./hello_1_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyPc75yXL526"
   },
   "source": [
    "## 2. To add two vectors of size 100 and 20000 and analyze the performance comparison between cpu and gpu processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZScCWhmRQ5rB"
   },
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 160782,
     "status": "ok",
     "timestamp": 1713244657693,
     "user": {
      "displayName": "SAL19IT 6023kavanmistry",
      "userId": "03477421122245380763"
     },
     "user_tz": -330
    },
    "id": "ZcvJganBVAHV",
    "outputId": "50f2e4b3-022a-4251-ab19-e1643a3b73f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycuda in d:\\anaconda\\envs\\tf\\lib\\site-packages (2024.1)\n",
      "Requirement already satisfied: pytools>=2011.2 in d:\\anaconda\\envs\\tf\\lib\\site-packages (from pycuda) (2024.1.1)\n",
      "Requirement already satisfied: appdirs>=1.4.0 in d:\\anaconda\\envs\\tf\\lib\\site-packages (from pycuda) (1.4.4)\n",
      "Requirement already satisfied: mako in d:\\anaconda\\envs\\tf\\lib\\site-packages (from pycuda) (1.3.3)\n",
      "Requirement already satisfied: platformdirs>=2.2.0 in d:\\anaconda\\envs\\tf\\lib\\site-packages (from pytools>=2011.2->pycuda) (3.10.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0 in d:\\anaconda\\envs\\tf\\lib\\site-packages (from pytools>=2011.2->pycuda) (4.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in d:\\anaconda\\envs\\tf\\lib\\site-packages (from mako->pycuda) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install pycuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 654,
     "status": "ok",
     "timestamp": 1713244658344,
     "user": {
      "displayName": "SAL19IT 6023kavanmistry",
      "userId": "03477421122245380763"
     },
     "user_tz": -330
    },
    "id": "CixF6vnyL4os"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\tf\\lib\\site-packages\\pycuda\\driver.py:45: UserWarning: Unable to discover CUDA installation directory while attempting to add it to Python's DLL path. Either set the 'CUDA_PATH' environment variable or ensure that 'nvcc.exe' is on the path.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1713244658344,
     "user": {
      "displayName": "SAL19IT 6023kavanmistry",
      "userId": "03477421122245380763"
     },
     "user_tz": -330
    },
    "id": "2vegVoEzPO5e"
   },
   "outputs": [],
   "source": [
    "# CUDA kernel function to add two vectors\n",
    "cuda_kernel_code = \"\"\"\n",
    "__global__ void vector_add(float *a, float *b, float *c, int n) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < n) {\n",
    "        c[i] = a[i] + b[i];\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 633,
     "status": "ok",
     "timestamp": 1713244658975,
     "user": {
      "displayName": "SAL19IT 6023kavanmistry",
      "userId": "03477421122245380763"
     },
     "user_tz": -330
    },
    "id": "g2UkVmumPO0g"
   },
   "outputs": [
    {
     "ename": "ExecError",
     "evalue": "error invoking 'nvcc --version': [WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mD:\\anaconda\\envs\\tf\\lib\\site-packages\\pytools\\__init__.py:724\u001b[0m, in \u001b[0;36mmemoize.<locals>._decorator.<locals>.wrapper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    723\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 724\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_memoize_dic\u001b[49m[args]  \u001b[38;5;66;03m# noqa: E501 # pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m    726\u001b[0m     \u001b[38;5;66;03m# _memoize_dic doesn't exist yet.\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute '_memoize_dic'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mD:\\anaconda\\envs\\tf\\lib\\site-packages\\pytools\\prefork.py:46\u001b[0m, in \u001b[0;36mDirectForker.call_capture_output\u001b[1;34m(cmdline, cwd, error_on_nonzero)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 46\u001b[0m     popen \u001b[38;5;241m=\u001b[39m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmdline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     stdout_data, stderr_data \u001b[38;5;241m=\u001b[39m popen\u001b[38;5;241m.\u001b[39mcommunicate()\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\tf\\lib\\subprocess.py:971\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[0;32m    968\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m    969\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m--> 971\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\tf\\lib\\subprocess.py:1456\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1455\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1456\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1457\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1458\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1459\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1460\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1461\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1462\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1463\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1466\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1469\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1470\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mExecError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Compile the CUDA kernel code\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m cuda_module \u001b[38;5;241m=\u001b[39m \u001b[43mSourceModule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcuda_kernel_code\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Get a reference to the CUDA kernel function\u001b[39;00m\n\u001b[0;32m      5\u001b[0m vector_add_cuda \u001b[38;5;241m=\u001b[39m cuda_module\u001b[38;5;241m.\u001b[39mget_function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvector_add\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\tf\\lib\\site-packages\\pycuda\\compiler.py:355\u001b[0m, in \u001b[0;36mSourceModule.__init__\u001b[1;34m(self, source, nvcc, options, keep, no_extern_c, arch, code, cache_dir, include_dirs)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    343\u001b[0m     source,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    351\u001b[0m     include_dirs\u001b[38;5;241m=\u001b[39m[],\n\u001b[0;32m    352\u001b[0m ):\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_arch(arch)\n\u001b[1;32m--> 355\u001b[0m     cubin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnvcc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mno_extern_c\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m        \u001b[49m\u001b[43march\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_dirs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpycuda\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdriver\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_from_buffer\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule \u001b[38;5;241m=\u001b[39m module_from_buffer(cubin)\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\tf\\lib\\site-packages\\pycuda\\compiler.py:304\u001b[0m, in \u001b[0;36mcompile\u001b[1;34m(source, nvcc, options, keep, no_extern_c, arch, code, cache_dir, include_dirs, target)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m include_dirs:\n\u001b[0;32m    302\u001b[0m     options\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-I\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m i)\n\u001b[1;32m--> 304\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompile_plain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnvcc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\tf\\lib\\site-packages\\pycuda\\compiler.py:96\u001b[0m, in \u001b[0;36mcompile_plain\u001b[1;34m(source, options, keep, nvcc, cache_dir, target)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m option \u001b[38;5;129;01min\u001b[39;00m options:\n\u001b[0;32m     95\u001b[0m     checksum\u001b[38;5;241m.\u001b[39mupdate(option\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m---> 96\u001b[0m checksum\u001b[38;5;241m.\u001b[39mupdate(\u001b[43mget_nvcc_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnvcc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpycuda\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcharacterize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m platform_bits\n\u001b[0;32m     99\u001b[0m checksum\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mstr\u001b[39m(platform_bits())\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\tf\\lib\\site-packages\\pytools\\__init__.py:727\u001b[0m, in \u001b[0;36mmemoize.<locals>._decorator.<locals>.wrapper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    724\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func\u001b[38;5;241m.\u001b[39m_memoize_dic[args]  \u001b[38;5;66;03m# noqa: E501 # pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m    726\u001b[0m     \u001b[38;5;66;03m# _memoize_dic doesn't exist yet.\u001b[39;00m\n\u001b[1;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    728\u001b[0m     func\u001b[38;5;241m.\u001b[39m_memoize_dic \u001b[38;5;241m=\u001b[39m {args: result}  \u001b[38;5;66;03m# noqa: E501 # pylint:disable=protected-access\u001b[39;00m\n\u001b[0;32m    729\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\tf\\lib\\site-packages\\pycuda\\compiler.py:16\u001b[0m, in \u001b[0;36mget_nvcc_version\u001b[1;34m(nvcc)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;129m@memoize\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_nvcc_version\u001b[39m(nvcc):\n\u001b[0;32m     15\u001b[0m     cmdline \u001b[38;5;241m=\u001b[39m [nvcc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--version\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 16\u001b[0m     result, stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mcall_capture_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmdline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stdout:\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warn\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\tf\\lib\\site-packages\\pytools\\prefork.py:221\u001b[0m, in \u001b[0;36mcall_capture_output\u001b[1;34m(cmdline, cwd, error_on_nonzero)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_capture_output\u001b[39m(cmdline, cwd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, error_on_nonzero\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_capture_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmdline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_on_nonzero\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\tf\\lib\\site-packages\\pytools\\prefork.py:58\u001b[0m, in \u001b[0;36mDirectForker.call_capture_output\u001b[1;34m(cmdline, cwd, error_on_nonzero)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m popen\u001b[38;5;241m.\u001b[39mreturncode, stdout_data, stderr_data\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ExecError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror invoking \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(cmdline), e))\n",
      "\u001b[1;31mExecError\u001b[0m: error invoking 'nvcc --version': [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "# Compile the CUDA kernel code\n",
    "cuda_module = SourceModule(cuda_kernel_code)\n",
    "\n",
    "# Get a reference to the CUDA kernel function\n",
    "vector_add_cuda = cuda_module.get_function(\"vector_add\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1713244658975,
     "user": {
      "displayName": "SAL19IT 6023kavanmistry",
      "userId": "03477421122245380763"
     },
     "user_tz": -330
    },
    "id": "XJnuci7QPOug"
   },
   "outputs": [],
   "source": [
    "def vector_add_gpu(a, b):\n",
    "    n = a.size\n",
    "\n",
    "    # Create device arrays\n",
    "    a_gpu = cuda.mem_alloc(a.nbytes)\n",
    "    b_gpu = cuda.mem_alloc(b.nbytes)\n",
    "    c_gpu = cuda.mem_alloc(b.nbytes)\n",
    "\n",
    "    # Copy data to device\n",
    "    cuda.memcpy_htod(a_gpu, a)\n",
    "    cuda.memcpy_htod(b_gpu, b)\n",
    "\n",
    "    # Define block and grid dimensions\n",
    "    block_dim = (256, 1, 1)\n",
    "    grid_dim = ((n + block_dim[0] - 1) // block_dim[0], 1)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Launch the CUDA kernel\n",
    "    vector_add_cuda(a_gpu, b_gpu, c_gpu, np.int32(n), block=block_dim, grid=grid_dim)\n",
    "\n",
    "    # Synchronize threads to ensure all output is calculated\n",
    "    cuda.Context.synchronize()\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Copy result back to host\n",
    "    c = np.empty_like(a)\n",
    "    cuda.memcpy_dtoh(c, c_gpu)\n",
    "\n",
    "    return c, end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1713244658976,
     "user": {
      "displayName": "SAL19IT 6023kavanmistry",
      "userId": "03477421122245380763"
     },
     "user_tz": -330
    },
    "id": "nOMGUEfRPOrp"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vector_add_gpu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(vector_size_2)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Perform vector addition on GPU\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m result_gpu1, gpu_time1 \u001b[38;5;241m=\u001b[39m \u001b[43mvector_add_gpu\u001b[49m(a[:vector_size_1], b[:vector_size_1])\n\u001b[0;32m      8\u001b[0m result_gpu2, gpu_time2 \u001b[38;5;241m=\u001b[39m vector_add_gpu(a[:vector_size_2], b[:vector_size_2])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vector_add_gpu' is not defined"
     ]
    }
   ],
   "source": [
    "vector_size_1 = 100\n",
    "vector_size_2 = 20000\n",
    "a = np.random.randn(vector_size_2).astype(np.float32)\n",
    "b = np.random.randn(vector_size_2).astype(np.float32)\n",
    "\n",
    "# Perform vector addition on GPU\n",
    "result_gpu1, gpu_time1 = vector_add_gpu(a[:vector_size_1], b[:vector_size_1])\n",
    "result_gpu2, gpu_time2 = vector_add_gpu(a[:vector_size_2], b[:vector_size_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1713244658977,
     "user": {
      "displayName": "SAL19IT 6023kavanmistry",
      "userId": "03477421122245380763"
     },
     "user_tz": -330
    },
    "id": "RPZ3W9TpPOpE",
    "outputId": "689329bc-4948-4807-f30b-744ff0309155"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector addition of size 100 on GPU took 0.0007691383361816406 seconds.\n",
      "Vector addition of size 20000 on GPU took 7.128715515136719e-05 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(\"Vector addition of size\", vector_size_1, \"on GPU took\", gpu_time1, \"seconds.\")\n",
    "\n",
    "print(\"Vector addition of size\", vector_size_2, \"on GPU took\", gpu_time2, \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjE0ERXeXL_W"
   },
   "source": [
    "* Vector addition of size 100 on GPU took 0.0007691383361816406 seconds.\n",
    "* Vector addition of size 20000 on GPU took 7.128715515136719e-05 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvEheCs9QnP6"
   },
   "source": [
    "### CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1713245356612,
     "user": {
      "displayName": "SAL19IT 6023kavanmistry",
      "userId": "03477421122245380763"
     },
     "user_tz": -330
    },
    "id": "nO6v1mvxQPAX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1713245356612,
     "user": {
      "displayName": "SAL19IT 6023kavanmistry",
      "userId": "03477421122245380763"
     },
     "user_tz": -330
    },
    "id": "zo1Vg8WJQO8b"
   },
   "outputs": [],
   "source": [
    "def vector_add_cpu(a, b):\n",
    "    start_time = time.time()\n",
    "    result = a + b\n",
    "    end_time = time.time()\n",
    "    return result, end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1713245356612,
     "user": {
      "displayName": "SAL19IT 6023kavanmistry",
      "userId": "03477421122245380763"
     },
     "user_tz": -330
    },
    "id": "ylRC8U3yQO5m"
   },
   "outputs": [],
   "source": [
    "vector_size_1 = 100\n",
    "vector_size_2 = 20000\n",
    "a = np.random.randn(vector_size_2).astype(np.float32)\n",
    "b = np.random.randn(vector_size_2).astype(np.float32)\n",
    "\n",
    "# Perform vector addition on CPU\n",
    "result_cpu1, cpu_time1 = vector_add_cpu(a[:vector_size_1], b[:vector_size_1])\n",
    "result_cpu2, cpu_time2 = vector_add_cpu(a[:vector_size_2], b[:vector_size_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1713245356612,
     "user": {
      "displayName": "SAL19IT 6023kavanmistry",
      "userId": "03477421122245380763"
     },
     "user_tz": -330
    },
    "id": "qvyOvmPVREnC",
    "outputId": "3be1d0e6-3160-4326-eaea-659aa0855a63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector addition of size 100 on CPU took 4.649162292480469e-05 seconds.\n",
      "Vector addition of size 20000 on CPU took 8.153915405273438e-05 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(\"Vector addition of size\", vector_size_1, \"on CPU took\", cpu_time1, \"seconds.\")\n",
    "print(\"Vector addition of size\", vector_size_2, \"on CPU took\", cpu_time2, \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfAueaUNTxNI"
   },
   "source": [
    "* Vector addition of size 100 on CPU took 2.384185791015625e-05 seconds.\n",
    "* Vector addition of size 20000 on CPU took 1.9788742065429688e-05 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlFZnJPSMCNU"
   },
   "source": [
    "## 3. To multply two matrix of size 20 X 20 and 1024 X 1024 analyze the performance comparison between cpu and gpu processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktQT63sESAbG"
   },
   "source": [
    "* GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1713244658977,
     "user": {
      "displayName": "SAL19IT 6023kavanmistry",
      "userId": "03477421122245380763"
     },
     "user_tz": -330
    },
    "id": "tJV-oQtaMJjs"
   },
   "outputs": [],
   "source": [
    "def matrix_multiply_gpu(a, b):\n",
    "    # Define CUDA kernel code for matrix multiplication\n",
    "    cuda_code = \"\"\"\n",
    "    __global__ void matrix_multiply(float *a, float *b, float *c, int n) {\n",
    "        int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "        int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "        if (row < n && col < n) {\n",
    "            float sum = 0.0;\n",
    "            for (int i = 0; i < n; ++i) {\n",
    "                sum += a[row * n + i] * b[i * n + col];\n",
    "            }\n",
    "            c[row * n + col] = sum;\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    # Compile CUDA kernel code\n",
    "    mod = SourceModule(cuda_code)\n",
    "\n",
    "    # Get kernel function\n",
    "    matrix_multiply_cuda = mod.get_function(\"matrix_multiply\")\n",
    "\n",
    "    # Allocate memory on device\n",
    "    a_gpu = cuda.mem_alloc(a.nbytes)\n",
    "    b_gpu = cuda.mem_alloc(b.nbytes)\n",
    "    c_gpu = cuda.mem_alloc(a.nbytes)\n",
    "\n",
    "    # Copy input matrices to device\n",
    "    cuda.memcpy_htod(a_gpu, a)\n",
    "    cuda.memcpy_htod(b_gpu, b)\n",
    "\n",
    "    # Define grid and block dimensions\n",
    "    block_size = (16, 16, 1)\n",
    "    grid_size = ((a.shape[1] + block_size[0] - 1) // block_size[0], (a.shape[0] + block_size[1] - 1) // block_size[1], 1)\n",
    "\n",
    "    # Call CUDA kernel\n",
    "    matrix_multiply_cuda(a_gpu, b_gpu, c_gpu, np.int32(a.shape[0]), block=block_size, grid=grid_size)\n",
    "\n",
    "    # Copy result back to host\n",
    "    c = np.empty_like(a)\n",
    "    cuda.memcpy_dtoh(c, c_gpu)\n",
    "\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1713244658977,
     "user": {
      "displayName": "SAL19IT 6023kavanmistry",
      "userId": "03477421122245380763"
     },
     "user_tz": -330
    },
    "id": "MH0PTY5uR93A"
   },
   "outputs": [],
   "source": [
    "# Function to generate random matrices\n",
    "def generate_random_matrix(rows, cols):\n",
    "    return np.random.rand(rows, cols).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 552,
     "status": "ok",
     "timestamp": 1713244694005,
     "user": {
      "displayName": "SAL19IT 6023kavanmistry",
      "userId": "03477421122245380763"
     },
     "user_tz": -330
    },
    "id": "ARcmhxX4R9hH"
   },
   "outputs": [],
   "source": [
    "# Function to measure time taken for matrix multiplication\n",
    "def measure_time(matrix_size, func, *args):\n",
    "    start_time = time.time()\n",
    "    result = func(*args)\n",
    "    end_time = time.time()\n",
    "    return result, end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 390,
     "status": "ok",
     "timestamp": 1713244713650,
     "user": {
      "displayName": "SAL19IT 6023kavanmistry",
      "userId": "03477421122245380763"
     },
     "user_tz": -330
    },
    "id": "wYUVcf1TV3NU"
   },
   "outputs": [],
   "source": [
    "# Sizes of matrices to be multiplied\n",
    "matrix_sizes = [(20, 20), (1024, 1024)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1038,
     "status": "ok",
     "timestamp": 1713244735001,
     "user": {
      "displayName": "SAL19IT 6023kavanmistry",
      "userId": "03477421122245380763"
     },
     "user_tz": -330
    },
    "id": "ZhDhR0szV3CK",
    "outputId": "ce0d6da2-98fa-4c6a-936b-80f85a3e7f4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matrix size: (20, 20)\n",
      "GPU time: 0.703994 seconds\n",
      "\n",
      "Matrix size: (1024, 1024)\n",
      "GPU time: 0.014648 seconds\n"
     ]
    }
   ],
   "source": [
    "for size in matrix_sizes:\n",
    "    print(f\"\\nMatrix size: {size}\")\n",
    "    a = generate_random_matrix(*size)\n",
    "    b = generate_random_matrix(*size)\n",
    "\n",
    "    # GPU matrix multiplication\n",
    "    gpu_result, gpu_time = measure_time(size, matrix_multiply_gpu, a, b)\n",
    "    print(f\"GPU time: {gpu_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "840b6ln5XRuy"
   },
   "source": [
    "\n",
    "* Matrix size: (20, 20)\n",
    "* GPU time: 0.703994 seconds\n",
    "\n",
    "* Matrix size: (1024, 1024)\n",
    "* GPU time: 0.014648 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1qyw1pBSImQ"
   },
   "source": [
    "* CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 790,
     "status": "ok",
     "timestamp": 1713245402800,
     "user": {
      "displayName": "SAL19IT 6023kavanmistry",
      "userId": "03477421122245380763"
     },
     "user_tz": -330
    },
    "id": "iodrO8o3R9c8"
   },
   "outputs": [],
   "source": [
    "def multiply_matrices_cpu(a, b):\n",
    "    return np.dot(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1713245402800,
     "user": {
      "displayName": "SAL19IT 6023kavanmistry",
      "userId": "03477421122245380763"
     },
     "user_tz": -330
    },
    "id": "YBPrjBRTR9Z2"
   },
   "outputs": [],
   "source": [
    "def compare_performance_multiply_matrices_cpu():\n",
    "    # Generate random input matrices\n",
    "    a = np.random.rand(1024, 1024)\n",
    "    b = np.random.rand(1024, 1024)\n",
    "    c = np.random.rand(20, 20)\n",
    "    d = np.random.rand(20, 20)\n",
    "\n",
    "    # CPU processing\n",
    "    start_time_cpu1 = time.time()\n",
    "    result_cpu1 = multiply_matrices_cpu(a, b)\n",
    "    end_time_cpu1 = time.time()\n",
    "\n",
    "    start_time_cpu2 = time.time()\n",
    "    result_cpu2 = multiply_matrices_cpu(c, d)\n",
    "    end_time_cpu2 = time.time()\n",
    "\n",
    "    # Print performance comparison\n",
    "    print(\"Performance Comparison for Multiplying Two Matrices (CPU):\")\n",
    "    print(\"CPU Time for 1024:\", end_time_cpu1 - start_time_cpu1, \"seconds\")\n",
    "    print(\"CPU Time for 20:\", end_time_cpu2 - start_time_cpu2, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1713245402800,
     "user": {
      "displayName": "SAL19IT 6023kavanmistry",
      "userId": "03477421122245380763"
     },
     "user_tz": -330
    },
    "id": "2npE0fjiSSlR",
    "outputId": "762a3756-d7ad-49b8-d553-8121ded7d8e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Comparison for Multiplying Two Matrices (CPU):\n",
      "CPU Time for 1024: 0.08771014213562012 seconds\n",
      "CPU Time for 20: 0.0022869110107421875 seconds\n"
     ]
    }
   ],
   "source": [
    "compare_performance_multiply_matrices_cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-K4-CpFT46o"
   },
   "source": [
    "* CPU Time for 1024: 0.12308359146118164 seconds\n",
    "* CPU Time for 20: 0.0019140243530273438 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOLoudChMMdC"
   },
   "source": [
    "## 4. To obtain CUDA device information and print the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 391,
     "status": "ok",
     "timestamp": 1713244872779,
     "user": {
      "displayName": "SAL19IT 6023kavanmistry",
      "userId": "03477421122245380763"
     },
     "user_tz": -330
    },
    "id": "-ZHjK0EQMwA0",
    "outputId": "c67dbc7c-3e49-404a-c700-4f1d241722e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CUDA devices: 1\n",
      "\n",
      "CUDA Device: 0\n",
      "  Name: Tesla T4\n",
      "  Compute Capability: (7, 5)\n",
      "  Total Memory: 14.74810791015625 GB\n",
      "  Max Threads per Block: 1024\n",
      "  Multiprocessor Count: 40\n",
      "  Clock Rate: 1.59 GHz\n"
     ]
    }
   ],
   "source": [
    "import pycuda.driver as cuda\n",
    "\n",
    "# Initialize PyCUDA\n",
    "cuda.init()\n",
    "\n",
    "# Get the number of CUDA devices\n",
    "num_devices = cuda.Device.count()\n",
    "\n",
    "print(\"Number of CUDA devices:\", num_devices)\n",
    "\n",
    "# Iterate over each CUDA device and print its properties\n",
    "for i in range(num_devices):\n",
    "    device = cuda.Device(i)\n",
    "    print(\"\\nCUDA Device:\", i)\n",
    "    print(\"  Name:\", device.name())\n",
    "    print(\"  Compute Capability:\", device.compute_capability())\n",
    "    print(\"  Total Memory:\", device.total_memory() / (1024 ** 3), \"GB\")\n",
    "    print(\"  Max Threads per Block:\", device.max_threads_per_block)\n",
    "    print(\"  Multiprocessor Count:\", device.multiprocessor_count)\n",
    "    print(\"  Clock Rate:\", device.clock_rate / 1e6, \"GHz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zk5911ZAXaCS"
   },
   "source": [
    "Number of CUDA devices: 1\n",
    "\n",
    "CUDA Device: 0\n",
    "  * Name: Tesla T4\n",
    "  * Compute Capability: (7, 5)\n",
    "  * Total Memory: 14.74810791015625 GB\n",
    "  * Max Threads per Block: 1024\n",
    "  * Multiprocessor Count: 40\n",
    "  * Clock Rate: 1.59 GHz"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOpZWnWSghtVkL2rGofeLpu",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
